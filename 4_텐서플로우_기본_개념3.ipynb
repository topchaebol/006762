{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4. 텐서플로우 기본 개념.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/topchaebol/006762/blob/master/4_%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0_%EA%B8%B0%EB%B3%B8_%EA%B0%9C%EB%85%903.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSs1oJmrpjOl",
        "colab_type": "text"
      },
      "source": [
        "# 1 간단하게 그래프 만들고 실행하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZFO9YpdEpdrI",
        "colab_type": "code",
        "outputId": "7cc51dfe-648c-43d0-d210-c89d1ef297b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 그래프 생성\n",
        "matrix1 = tf.constant([[3., 3.]])\n",
        "matrix2 = tf.constant([[2.],[2.]])\n",
        "product = tf.matmul(matrix1, matrix2)\n",
        "\n",
        "# 그래프 실행\n",
        "sess = tf.Session()\n",
        "result = sess.run(product)\n",
        "sess.close()\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[12.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82lNVLEQqlz1",
        "colab_type": "text"
      },
      "source": [
        "# 2 텐서플로우의 상수, 변수, 플레이스홀더"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBuG2Jpcpi-u",
        "colab_type": "code",
        "outputId": "516574ce-9102-4bec-a06c-351683c410d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "input1 = tf.placeholder(tf.float32)\n",
        "input2 = tf.placeholder(tf.float32)\n",
        "output = input1 * input2\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(output, feed_dict={\n",
        "                                   input1:[7.],\n",
        "                                   input2:[2.]}))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[14.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owzUcDTQs-wL",
        "colab_type": "text"
      },
      "source": [
        "# 3 텐서플로우를 활용한 단층망 만들기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn3EdlDEszP7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAVP7paHtTCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# 네트워크에 필요한 변수\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "W = tf.Variable(tf.zeros([784, 10]))\n",
        "b = tf.Variable(tf.zeros([10]))\n",
        "\n",
        "# 네트워크 생성\n",
        "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# 코스트 구하기\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "# 최적화 알고리즘\n",
        "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnJOUvMpwW24",
        "colab_type": "code",
        "outputId": "1e497211-3045-4004-b187-5573e1231d92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  \n",
        "  for i in range(1000):\n",
        "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
        "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
        "    \n",
        "  # 모델 평가(몇 개가 맞았는지 확인)\n",
        "  correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "  print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9164\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmVt3-YSfy3L",
        "colab_type": "text"
      },
      "source": [
        "# 4 텐서플로우로 멀티레이어 구현하기\n",
        "다음 모델을 만족하는 네트워크를 구성하시오\n",
        "- mnist 데이터셋을 사용하여 목표 정확률 : 96%\n",
        "- 입력부터 출력, 히든 레이어의 개수: 784 392 196 98 10(각 계층에 RELU 적용)\n",
        "- GPU를 사용하여 학습\n",
        "- 입력 레이어부터 히든 레이어, 출력 레이어까지 784, 392, 196, 98, 10의 계층 사용\n",
        "- epoch: 3000\n",
        "- batch size: 16\n",
        "- 100번의 epoch마다 정확률 출력\n",
        "- Adam 옵티마이저를 사용\n",
        "- learning rate: 0.001\n",
        "- 시간에 따른 테스트셋의 정확률과 트레이닝 셋의 정확률을 각각 기록하고 그래프로 시각화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqRX-LS5xQ4P",
        "colab_type": "code",
        "outputId": "08dbfa23-eb27-4903-a43b-3be806aa2980",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# 네트워크 그래프에 필요한 변수 선언\n",
        "x = tf.placeholder(tf.float32, [None, 784]) # 입력에 필요한 데이터를 넣을 플레이스 홀더\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 392], \n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.zeros([392])) # 연산에 필요한 편향치\n",
        "W2 = tf.get_variable(\"W2\", shape=[392, 196], \n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.zeros([196])) # 연산에 필요한 편향치\n",
        "W3 = tf.get_variable(\"W3\", shape=[196, 98], \n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.zeros([98])) # 연산에 필요한 편향치\n",
        "W4 = tf.get_variable(\"W4\", shape=[98, 10], \n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.zeros([10])) # 연산에 필요한 편향치\n",
        "\n",
        "# 네트워크 생성\n",
        "l1 = tf.nn.relu(tf.matmul(x, W1) + b1)\n",
        "l2 = tf.nn.relu(tf.matmul(l1, W2) + b2)\n",
        "l3 = tf.nn.relu(tf.matmul(l2, W3) + b3)\n",
        "y = tf.nn.softmax(tf.matmul(l3, W4) + b4)\n",
        "y_ = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "# 코스트 구하기\n",
        "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
        "\n",
        "# 최적화 알고리즘\n",
        "#train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
        "train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)\n",
        "\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.device('/device:GPU:0'):\n",
        "\n",
        "  with tf.Session() as sess:\n",
        "    sess.run(init)\n",
        "    \n",
        "    train_ce = dict()\n",
        "    test_ce = dict()\n",
        "    train_acc = dict()\n",
        "    test_acc = dict()\n",
        "\n",
        "    for i in range(3000):\n",
        "      batch_xs, batch_ys = mnist.train.next_batch(16)\n",
        "      sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
        "      \n",
        "      if(i%100==99):\n",
        "        count += \n",
        "        dict_history['train_ce'] = sess.run(cross_entropy, \n",
        "                                            feed_dict={x: mnist.train.images, y_: mnist.train.labels})\n",
        "        dict_history['test_ce'] = sess.run(cross_entropy, \n",
        "                                           feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
        "        dict_history['train_acc'] = sess.run(accuracy, \n",
        "                                             feed_dict={x: mnist.train.images, y_: mnist.train.labels})\n",
        "        dict_history['test_acc'] = sess.run(accuracy, \n",
        "                                            feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n",
        "        \n",
        "    # 모델 평가(몇 개가 맞았는지 확인)\n",
        "    print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0705 01:38:59.782340 139928062580608 deprecation.py:323] From <ipython-input-1-8cf27f99ce7b>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "W0705 01:38:59.783814 139928062580608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please write your own downloading logic.\n",
            "W0705 01:38:59.784804 139928062580608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0705 01:39:00.025647 139928062580608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.data to implement this functionality.\n",
            "W0705 01:39:00.028222 139928062580608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use tf.one_hot on tensors.\n",
            "W0705 01:39:00.077609 139928062580608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "0.952\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OL108tzsyR-w",
        "colab_type": "code",
        "outputId": "9b2a19c0-e449-4063-e921-d9e44037e6f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(dict_history)\n",
        "df\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-5ccd623860e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    390\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[1;32m    391\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[0;34m(data, index, columns, dtype)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0marrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# figure out the index, if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mraw_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             raise ValueError('If using all scalar values, you must pass'\n\u001b[0m\u001b[1;32m    309\u001b[0m                              ' an index')\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueFt6ThDhIE0",
        "colab_type": "code",
        "outputId": "d45db0ad-9d34-480f-bdd2-e0c99eeda5bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "LOG_DIR = 'drive/data/tb_logs'\n",
        "\t\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\t\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "\t\n",
        "get_ipython().system_raw('tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'.format(LOG_DIR))\n",
        "\t\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\t\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "\"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-05 00:31:07--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.201.75.180, 52.45.111.123, 52.207.111.186, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.201.75.180|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 17556757 (17M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  16.74M  20.7MB/s    in 0.8s    \n",
            "\n",
            "2019-07-05 00:31:13 (20.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [17556757/17556757]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://b7df7b7f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA9qGPfwhIK_",
        "colab_type": "code",
        "outputId": "7824e6e7-47a2-4535-b8a4-7dcc75b3b095",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "from keras.callbacks import TensorBoard\n",
        "import tensorflow as tf\n",
        "\n",
        "# See https://www.tensorflow.org/tutorials/using_gpu#allowing_gpu_memory_growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "tf_device = tf.device('/device:GPU:0')\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\t\n",
        "### input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\t\n",
        "### the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\t\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\t\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        "\t\n",
        "### convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                activation='relu',\n",
        "                input_shape=input_shape))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\t\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(),\n",
        "              metrics=['accuracy'])\n",
        "\t\n",
        "\t\n",
        "tbCallBack = TensorBoard(log_dir=LOG_DIR, \n",
        "                           histogram_freq=1,\n",
        "                           write_graph=True,\n",
        "                           write_grads=True,\n",
        "                           batch_size=batch_size,\n",
        "                           write_images=True)\n",
        "\t\n",
        "model.fit(x_train, y_train,\n",
        "            batch_size=batch_size,\n",
        "            epochs=epochs,\n",
        "            verbose=1,\n",
        "            validation_data=(x_test, y_test),\n",
        "            callbacks=[tbCallBack])\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0705 00:31:19.705982 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0705 00:31:19.707855 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0705 00:31:19.718993 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0705 00:31:19.755138 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0705 00:31:19.760015 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0705 00:31:19.769443 140167498225536 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0705 00:31:19.840216 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0705 00:31:19.849480 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0705 00:31:22.042778 140167498225536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0705 00:31:25.408417 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:796: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0705 00:31:25.606624 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:840: The name tf.summary.image is deprecated. Please use tf.compat.v1.summary.image instead.\n",
            "\n",
            "W0705 00:31:26.825769 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0705 00:31:26.828674 140167498225536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 8s 139us/step - loss: 0.2709 - acc: 0.9165 - val_loss: 0.0587 - val_acc: 0.9806\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0899 - acc: 0.9741 - val_loss: 0.0412 - val_acc: 0.9861\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 5s 78us/step - loss: 0.0657 - acc: 0.9805 - val_loss: 0.0339 - val_acc: 0.9885\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0548 - acc: 0.9839 - val_loss: 0.0297 - val_acc: 0.9901\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0455 - acc: 0.9862 - val_loss: 0.0296 - val_acc: 0.9904\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0417 - acc: 0.9878 - val_loss: 0.0270 - val_acc: 0.9915\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0379 - acc: 0.9885 - val_loss: 0.0263 - val_acc: 0.9914\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0334 - acc: 0.9901 - val_loss: 0.0235 - val_acc: 0.9923\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0312 - acc: 0.9908 - val_loss: 0.0280 - val_acc: 0.9905\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0293 - acc: 0.9910 - val_loss: 0.0238 - val_acc: 0.9923\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0274 - acc: 0.9915 - val_loss: 0.0265 - val_acc: 0.9920\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 5s 77us/step - loss: 0.0246 - acc: 0.9922 - val_loss: 0.0245 - val_acc: 0.9919\n",
            "Test loss: 0.024540694340224035\n",
            "Test accuracy: 0.9919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho46bZfshIRx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}